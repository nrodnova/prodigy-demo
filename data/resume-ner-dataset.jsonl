{"resume_id":1,"text":"4\nOct\nArjun Reddy\n9185551234 | arjun.reddy@themailpad.com | Irving, Texas\nMaster's in Management Information Systems | Texas A&M University-Commerce, TX\nBachelor in Computer Science | VNR Vignana Jyothi Institute of Engineering and Technology, Hyderabad, India"}
{"resume_id":2,"text":"LUCAS M\nBERNARD-\nDUPONT\n\nLUCASBERNARD@PROTONMAIL.COM\n\n(949) 423-8672\n\nSKILLS\n\nWords per minute typing: 60+\nMedical terminology\nCommunication and leadership skills\n\nEXPERIENCE\nMEDICAL SCRIBE @ OROVILLE HOSPITAL\nSept 2021 - Present\nExtended the ability of the provider to care for patients by completing\nbilling, medical notes, creating medical orders and answering patient\nquestions. I am leaving this job because I am not being given enough\nhours.\nINTERN AT UC DAVIS MEDICAL CENTER\nJan 1st 2020 to April 2020\nAssisted nursing staff in the accelerated access unit by dispatching them\nto patients' rooms. Maintained in-room supplies of consumables such as\ngloves and masks. The program was discontinued due to COVID-19.\nEDUCATION\nGENETICS AND GENOMICS (NO DEGREE EARNED)\nUC Irvine\nStudied advanced scientific practices in the field of genetics including\nlaboratory design and critical analysis of experimental data. Performed\nscientific inquiries into genomic data using data analysis. I am currently\ntaking 2 gap years to study medicine and study for my MCAT before\nreturning to UC Davis to complete my degree.\nVOLUNTEER EXPERIENCE OR LEADERSHIP\n-\nLed a scientific study for 4 years that was published.\n-\nElected president of bioethics club at UC Davis for 2 years.\n-\nCurrent Red Cross volunteer.\n-\nVolunteer mentor for 5 years at UC Davis GSP program for foster\nyouth seeking higher education.\n-\nVolunteer mentor for UC Davis BASC Program 2 years\n"}
{"resume_id":3,"text":"RYAN KALINOWSKI, M.S.\nData Scientist | Machine Learning | Healthcare Analytics\n\n(585) 305-7824             rkalinowski@cornell.edu\nLinkedIn\nSeattle, WA\n\nEXPERIENCE\n\nData Scientist II\n\n2/2023 - Present\nL.A. Care Health Plan\n*\nLed a project reducing a year-long data delay and safeguarding $15M annually in provider payments using deep learning, statistical\ncontrol charts, and randomized control trials for root cause analysis and resolution of insurance claim payments and denials\n*\nSignificantly mitigated the risk of critical regulatory program failure and business line disruption by leveraging state-of-the-art machine\nlearning to identify and intervene in ~200 multi-drug failing members with PharmDs\n*\nDeveloped Tableau dashboards and a Python repository for outlier analysis across business domains including claims processing\n*\nIdentified social determinants of health in insurance grievances by fine-tuning large language model (LLM) on sensitive, regulated data\n\nMachine Learning Research Assistant\n\n9/2021 - 12/2022\nTong Lab, Georgia Southern University\n*\nDesigned and implemented a Google Cloud Platform (GCP) environment for the lab, supporting high-compute PyTorch Lightning\nmodels that led to two major publications\n*\nProvided graduate students with in-depth instruction on algorithmic proof techniques, equipping them with the skills to excel in a highly\nrigorous course\n\nEpic Consultant\n\n1/2021 - 8/2021\nUniversity of Miami\n\nEpic Healthcare Analyst II\n\n9/2018 - 9/2020\nProvidence Healthcare\n*\nLed the roll-out of COVID-19 lab testing across the West Coast as part of a three-person team\n\nTechnical Engineer\n\n5/2017 - 6/2018\nEsker Inc.\n\nTechnical Engineer\n\n8/2015 - 5/2017\nEpic Systems\n*\nLed technical services across multiple initiatives including the award-winning implementation of a new case management module\n\nEDUCATION\n\nM.S. in Computer Science, summa cum laude\n\n9/2021 - 12/2022\nGeorgia Southern University\n\nStatesboro, GA\n*\nGPA: 4.0 | Focus Area: Machine Learning | PI: Yifan Zhou, PhD\n*\nReceived full scholarship with graduate stipend, won 1st and 2nd place in 2022 Student Research Symposium, and published two papers\n\nB.S. in Biology, magna cum laude\n\n8/2011 - 5/2015\nRochester Institute of Technology\n\nRochester, NY\n*\nGPA: 3.8 | Focus Area: Computational Neuroscience\n\nPUBLICATIONS\n\n*\nEnhanced Transformer Architectures for Multi-Region Air Quality Estimation Using Remote Sensor Data\n*\nSpatiotemporal Air Pollution Forecasting via Deep Neural Embeddings and Attention Mechanisms\n\nSKILLS\n\nPython, Java, SQL, PyTorch, Apache Spark, Machine Learning, Deep Learning, Git, Object-Oriented Programming,\nTransformers, Large Language Models, MLOps, Bayesian Statistics, Pandas, NumPy, Google Cloud Platform (GCP), A/B\nTesting, spaCy, NLTK, LangChain, XGBoost, scikit-learn, DataBricks, Tableau, REST API"}
{"resume_id":4,"text":"Arvada, CO\nLinkedIn\nMorgan Ellis, Ph.D.\nmorgan.ellis@gmail.com\n(415) 716-4823\nWORK EXPERIENCE\nSenior Data Scientist\nLumen Technologies\nRemote\nJun 2021 - Present\n- Summary -\n* Responsible for end-to-end AI / machine learning project lifecycles - including project scoping, sourcing and\ncleaning data, training and tuning models, and publishing automated reports.\n* Credited with numerous actionable insights leading to millions in financial savings and revenue increases.\n* Strong communication skills proven by frequent presentations to VPs and high-quality reports.\n- Project Highlights -\nCustomer Success Targeting Pipeline | Tech lead | Jun 2024 - Present\nRecommends sales motions for customer success representatives near the ends of customer and product lifecycles.\n* Achieved 10% conversion rate on recommended sales opportunities, directly linked to millions in revenue.\n* Synthesized financial data, account info, product specs, health score, and churn predictions using SQL,\nPySpark, Azure Databricks, and Azure Data Factory.\n* Posted automated monthly account targeting list for numerous downstream teams via Salesforce / Gainsight.\nRevenue Churn Prediction Model | Senior advisor and contributor | Aug 2024 - Present\nXGBoost classification model trained to predict decreases in revenue at customer level.\n* Enhanced model performance through balanced dataset creation, hyperparameter optimization, and evalu-\nation metric selection using scikit-learn, MLflow, Optuna, SHAP, and AutoML.\n* Migrated storage and compute from on-prem resources to Azure.\nCapital Actuals API | Tech lead | Apr 2023 - Sep 2024\nWeb interface allowing users to query capital actuals data for network buildout projects.\n* Published a self-service dataset to eliminate costly data pull requests using Docker and Kedro.\nUtility Usage Prediction | Tech lead | Jun 2021 - Oct 2022\nXGBoost regression model and reporting framework identifying outliers in electricity usage at building level.\n* Identified tens of thousands in unnecessary monthly utility spend.\n* Deduplicated and enhanced existing data using REST API and custom inventory classification.\nSKILLS AND TECHNOLOGIES\nData\nQuerying, Manipulation | Oracle SQL, MSSQL, PySpark, Hue, sqlalchemy, pandas, R\nStorage | Oracle Database, SQL Server, Anaplan, Hadoop Data Lake, DBeaver\nVisualization | PowerBI, seaborn, matplotlib\nML / AI\nModels | Regression and classification (XGBoost, LightGBM), time series, clustering\nLibraries | scikit-learn, pyspark.ml, Optuna, Prophet, SHAP, Azure AutoML\nDeployment, MLOps | Databricks Workflows, Airflow, Kedro, MLflow, Docker\nTools\nCoding, DevOps | Git, Github, VSCode, JupyterLab, Jenkins, Linux, Postman\nCloud | Azure Databricks, Azure Data Factory, Azure SQL, Azure Key Vault, BigQuery\nEDUCATION\nBoulder, CO\nColorado State University\nAug 2015 - May 2021\n* Ph.D. | Mathematics\n* Dissertation Title | Investigations into Stationary Fluid Dynamics on Non-Euclidean Geometries\n* Research Areas | Partial differential equations, Riemannian manifolds, Euler and Navier-Stokes equations\nEllensburg, WA\nWestern Washington University\nSep 2011 - Jun 2015\n* B.A.s | Mathematics and Secondary Mathematics Education - Summa Cum Laude"}
{"resume_id":5,"text":"SUMMARY\nRohan Desai\nData Scientist\nMedford, MA | (716) 384-7623 | rohan.desai@myjobsmails.com | LinkedIn\n\n*\nExperienced Data Scientist with 5 Years, specializing in machine learning, deep learning, and AI-driven risk assessment to\noptimize credit risk predictions and fraud detection through Hybrid AI models (LSTMs, Transformers, XGBoost, and Random\nForests) for improved decision-making.\n*\nProficient in Python, SQL, Graph Networks and AWS, using advanced algorithms to extract insights from complex datasets,\nimproving business processes, and delivering actionable insights to drive customer retention and operational efficiency.\n*\nDeep knowledge of Databricks and Spark to scale feature engineering for millions of customers, and CI/CD pipelines using\nBitbucket, Jenkins, JIRA, and CA Automation (Autosys), improving customer experience and business outcomes.\n*\nExpertise in building and implementing predictive models and using data visualization tools like Tableau and Power BI, to create\nreal-time insights and optimize key business processes, resulting in more accurate forecasts and better decision-making.\nEXPERIENCE\nPNC, USA | Data Scientist\nJan 2024 - Current\n*\nDeveloped Hybrid AI models by combining Deep Learning (LSTMs, Transformers) and Traditional ML (XGBoost, Random\nForest) to optimize credit risk predictions and fraud detection, increasing detection rates by 18%.\n*\nDeveloped fraud detection models using graph-based anomaly detection, reducing fraudulent transactions by 20% by identifying\nunusual entity relationships in transaction networks.\n*\nIntegrated alternative data sources (transaction history, behavioral analytics, web interactions) into risk assessment models , leveraging\nDatabricks and Spark to scale feature engineering for millions of customers.\n*\nDeployed and monitored ML models in AWS (SageMaker, Lambda), ensuring low-latency fraud detection and credit risk assessments\nat scale.\n*\nDesigned and published visually rich, interactive Tableau dashboards to provide actionable insights into key financial metrics, aiding\nexecutive decision-making for investment strategies.\nTATA CONSULTANCY SERVICES, India | Data Engineer\nJune 2019 - May 2022\n*\nLed the development of a robust ETL infrastructure using IBM InfoSphere DataStage, integrating data from corporate and investment\nbanking divisions of two major U.S.-based banks, resulting in a 40% reduction in data processing time and improving data accessibility for\nreal-time analytics.\n*\nOptimized complex customer data extraction and analysis using Oracle SQL and Machine Learning (XGBoost, LightGBM),\nidentifying key financial performance metrics that increased customer engagement by 30% through targeted financial strategies.\n*\nEnhanced forecasting accuracy of financial risk models by 35% using Keras and TensorFlow, developing predictive models that\noptimized risk management strategies, empowering the bank to make 20% faster data-driven financial decisions while minimizing\nforecasting errors.\n*\nAutomated CI/CD pipelines using Bitbucket, Jenkins, JIRA, and CA Automation (Autosys), reducing deployment failures by 40%,\naccelerating ETL deployment cycles by 30%, and ensuring seamless job scheduling and version control for ETL workflows.\n*\nImplemented end-to-end testing automation for ETL workflows using TOSCA and Jenkins, leading to a 45% reduction in production\nbugs and ensuring a 99.9% uptime of critical data pipelines.\nInformative Web Solution, India | Data Scientist Intern\nFeb 2018- April 2019\n*\nAssisted in developing machine learning models using Python and scikit-learn to predict customer behavior and improve data accuracy.\n*\nSupported data cleaning tasks using SQL and Excel, helping the team streamline data workflows and improve reporting efficiency.\n*\nCollaborated with senior data scientists to analyze financial data, providing insights that helped optimize customer segmentation.\nTECHNICAL SKILS\nLanguage: Python, R, SQL, SAS, Java, Unix, C, HTML, CSS, Bootstrap 4\nStatistical Methods/ Visualization Tools: Hypothetical Testing, ANOVA, Time Series, Tableau, Power BI, Microsoft Excel\nMachine Learning: Regression analysis, Bayesian Method, Decision Tree, Random Forests, Support Vector Machine, Neural Network,\nSentiment Analysis, K-Means Clustering, KNN, Classification, SVM, Naive Bayes, Natural Language Processing (NLP), LLM, CNN,\nPackages: NumPy, Pandas, Matplotlib, SciPy, ggplot2, Scikit-Learn, PyTorch, TensorFlow, Keras, Spark\nCloud Technologies / Database / IDEs: AWS, GCP, Azure, MySQL, SQL Server, Oracle, MongoDB\nMethodologies/Software/Other Skills: Jira, Data Cleaning, Data Wrangling, Critical Thinking, Communication Skills, Presentati on Skills,\nProblem-solving, Decision-Making, EDA, Communication Skills, Databricks, Data Visualization, Predictive Analytics, Pattern Recognition,\nJMP, Data Integrity, Data Science, Statistics, Statistical Analysis, Data Analytics, Data Modeling, Big Query, Snowflake, SDLC, Agile, Waterfall\nEDUCATION\nNortheastern University, Boston, MA, USA\nMaster of Science, Engineering Science (Robotics)\nUniversity of Pune, India\nBachelor of Engineering, Computer Engineering\nCERTIFICATION\nMachine Learning by Andrew NG - Coursera"}
{"resume_id":6,"text":"Evan Marlow\nSenior Data Scientist\nevan.marlow.lol@gmail.com\n\n4086004829\n\nwww.linkedin.com/in/evan-lol\n\nSan Francisco, CA\n\nSummary\nAccomplished Data Scientist with a strong track record of developing AI-driven solutions in biotechnology,\nhealthcare, and e-commerce. Expertise in designing and deploying advanced machine learning models, including\nNLP, deep learning, and predictive analytics, utilizing PyTorch, Scikit-learn, and AWS infrastructure. Skilled in\nworking with large language models (LLMs) such as GPT and BERT, retrieval-augmented generation (RAG), and\nfine-tuning AI models for enhanced performance. Proven ability to optimize ML pipelines, improve model\naccuracy, and automate data workflows to drive efficiency. Adept at collaborating cross-functionally with\nengineers, scientists, and product teams to deliver AI-powered insights that enhance user engagement, content\nmoderation, and campaign performance. Passionate about leveraging cutting-edge AI technologies to enable\ndata-driven decision-making and strategic growth.\nSkills\nProgramming Languages & Frameworks: Python | R | TensorFlow | PyTorch | Pandas | PySpark | SQL\nData Science & Machine Learning: Generative AI | Deep Learning | Natural Language Processing | Random Forest |\nSupport Vector Machines | Autoencoders | Predictive Modeling\nNatural Language Processing: Transformer | BERT | GPT-based models | RAG | SpaCy | NLTK | OpenAI API\nData Analysis & Visualization: Scikit-learn | Langchain | Matplotlib | Clustering | Latent Dirichlet Allocation (LDA) |\nFeature Extraction with OpenCV\nTools & Techniques: Retrieval-Augmented Generation | Regression Analysis | VAE Evaluation | Linear Modeling |\nTechniques Selective Search R-CNN | CNC-VAE | Text and Image Data Integration | Pipeline Development &\nOptimization\nCloud & DevOps: AWS | Azure Databricks | Docker | OpenShift | Streamlit\nProfessional Experience\nInvitae\nSenior Data Scientist\n09/2019 - Present  | San Francisco, CA\n*NLP Solutions Implementation: Spearheaded the design and development of natural language processing\n(NLP) solutions for Invitae's Virtual Genetic Information Assistant (GIA), enhancing user interaction and\ninformation retrieval through BERT, GPT-based models, and Retrieval-Augmented Generation (RAG)\ntechniques.\n*Hereditary Disease Risk Prediction Model: Engineered a predictive model and personalized recommendation\nsystem utilizing XGBoost, BioBERT, and GPT-based architectures, specifically tailored for assessing\nhereditary disease risks.\n*Accuracy Enhancement: Achieved a 17% improvement in predictive accuracy over traditional rule-based\nmethods by leveraging PyTorch, Scikit-learn, and Transformer-based models to rigorously analyze over 3\nmillion patient medical data points.\n*Variant Reclassification: Actively contributed to addressing Variants of Uncertain Significance (VUS) by\nreclassifying over 15,000 uncertain genetic variants. This was accomplished through LLM-powered\nfunctional genomics and allele frequency analysis, integrating deep learning and transformer-based models.\n*Advanced Variant Classification Model: Developed a sophisticated variant classification model utilizing\nLightGBM and deep learning-based functional modeling frameworks (PyTorch, CNNs, and Transformers) to\npredict the protein structure impacts of genetic variants. Integrated LLMs and RAG methodologies to\nenhance data-driven insights for variant classification.\nJohnson and Johnson\nMachine Learning Engineer\n01/2016 - 05/2019  | Menlo Park, CA\n*Development of AI Models for Healthcare Performance Analysis: Designed and implemented deep learning\nmodels using PyTorch and TensorFlow to analyze and predict healthcare performance metrics. Optimized\nCNN-based model architectures, improving predictive accuracy by 11%, with a focus on key biomedical\nfactors derived from extensive clinical data.\n*Large-Scale Data Pipeline Management on AWS: Developed and maintained scalable, distributed data\npipelines leveraging AWS infrastructure (ECS, Aurora, Lambda, Batch), all orchestrated via AWS CDK.\nIntegrated PySpark for efficient handling of massive datasets, facilitating streamlined feature extraction and\nmodel inference.\n*Advanced Data Analysis and Research: Applied state-of-the-art computer vision, statistical analysis, and ML\ntechniques to extract critical insights from medical datasets. Leveraged CNNs, OpenCV, Pandas, NumPy,\nScikit-learn, and Matplotlib, enhancing model training efficiency and prediction consistency.\n*Automation of Data Processing and Feature Engineering: Engineered automated data pipelines for ingestion,\npreprocessing, and storage using AWS Lambda, Batch, and PySpark, reducing data processing latency by\n20%. Designed real-time analytics workflows supporting healthcare performance tracking.\n*Cross-Functional Collaboration for AI Integration: Partnered with data scientists, healthcare professionals,\nand software engineers to integrate ML-driven insights into J&J's healthcare platforms. This collaboration\nresulted in enhanced real-time tracking tools, improving patient and provider decision-making.\nAmazon\nData Scientist\n11/2013 - 12/2015  | Seattle, WA\n*Customer Segmentation Optimization: Developed clustering models and behavioral analytics pipelines using\nPython, Scikit-learn, and SQL to segment Amazon customers based on engagement and purchasing behavior.\nThese insights refined marketing strategies, increasing targeted campaign effectiveness by 15%.\n*AI-Driven Content Moderation: Designed and implemented NLP-based models to automate spam detection,\npolicy violation identification, and abusive content moderation. This innovation reduced moderation\nresponse times by 20%, significantly easing the manual workload for moderation teams.\n*Data-Driven Growth Strategies: Conducted A/B testing and statistical analysis to evaluate the impact of\nplatform changes on customer behavior. Provided actionable insights that led to a 10% increase in customer\nretention, supporting key product and growth initiatives.\nEducation\nSan Francisco State University\nMaster of Science\n09/2007 - 09/2013"}
{"resume_id":7,"text":"Houston, TX\nafarooq.amina@gmail.com\nafarooq-amina-426568104/\nBorn July 29, 1988\nWORK EXPERIENCE\nMay 2024 - present\nStaff Data Scientist\nUdemy\n* Counterfactual/impact analysis\n- simulation of model output (demand and revenue) under different scenarios -\nconducted inference using bootstrapping.\n* pricing model improvements\n- used techniques like Poisson regression and Zero-inflated Poisson to account\nfor sparse data in demand prediction.\n- A/B testing and evaluation of time-based tests.\n* data pipeline debugging\n* identifying and fixing data quality issues in the model pipeline.\nJuly 2022 - April 2024\nSenior Data Scientist, Inference\nUdemy\n* revamped price optimization framework and productionalized model codes.\n- used techniques from panel data econometrics and ML (XGboost) to estimate\ndemand and optimize prices\n- deployed model into production.\n- monitored performance\n* debugging and modifying batch pipelines\n* model performance and query optimization.\n* causal inference: prototyped codes for implementation of synthetic control, synthetic\ndiff in diff and diff in diff to scale and standardize use of these methods within the\nlarger data science pod.\n* experiment design and evaluation\nJan 2021 - June 2022\nLead Economist\nThumbtack\n* causal inference with observational data - panel data econometrics\n* NLP (sentiment analysis) to understand and categorize the textual data in customer\nreviews.\n* customer segmentation using techniques like Latent Class Analysis and Decision\nTrees / nonparametric\n* Predictive modeling using Logistic regression and XGBoost to predict churn.\nMay 2020 - Dec 2020\nVice President, Economic and Financial Forecasting\nCitigroup\n* Time series econometrics: Vector Error Correction Models for sneario analysis under\nstress testing.\nEDUCATION\n2014 - 2020\nPhD Economics\nRice Institute, Houston, TX\n* Job Market Paper: \"Policy Incentives and Postsecondary Attainment: An Economic Analysis\"\n2012 - 2013\nMSc Economic Theory & Econometrics\nLyon School of Economics, France\n2011 - 2012\nM.Sc. Macroeconomic Policy & Financial Markets\nMadrid Graduate School of Economics, Spain\n2007 - 2009\nB.A. in Economics & Political Science, minor Mathematics, cum laude\nUniversity of Illinois, Urbana-Champaign, IL\nCERTIFICATIONS\nDecember 2024\nPython Essentials for MLOps\nDuke University (coursera)\ncredential ID 8ECAXX6BPSEX\nFebruary 2021\nPostgraduate certification, Artificial Intelligence Machine Learning\n(AIML)\nTexas McCombs School of Business\nTECHNICAL STACK\nProgramming\nPython, R, Fortran, Matlab, Julia, SQL, Latex.\nOthers\n* AWS, S3, Redshift, Airflow, Databricks, Hive, Github\n* Optimization\n* Econometric and statistical modeling\nResearch Papers\nPolicy Incentives and Postsecondary Attainment: An Economic Analysis - Job Market Paper\nIn 2015, the Texas Education Agency (TEA) in collaboration with the Texas Higher Edu-\ncation Coordinating Board (THECB) undertook an ambitious 60x30 TX plan as a part\nof its implementation of House Bill 22, introduced by the 85th Texas House of Repres-\nentatives to enhance public school accountability. The 60x30 TX is a higher education\nplan that focuses on attaining a postsecondary graduation rate of 60% amongst the\n25-34 age demographic in Texas by the year 2030. In this paper, I estimate the cost\nper high school graduate of attaining this target. I also compare the per student cost\nof this policy objective with that of two other counterfactual policies; the first being one\nin which community colleges are made tuition-free for all high school graduates and\nthe second being one in which public four-year college tuition is subsidized annually by\n$2,000 only for those who have completed an AA tuition free under the former policy.\nI find that a policy in which community colleges are made tuition free will increase the\npostsecondary graduation rate by age 29 to 26.6%, relative to a baseline of 22.6%, and\nwill cost $2,114 per student. I find that a $2,000 public four-year college subsidy for\nthose who have first completed an AA tuition free will boost the postsecondary gradu-\nation rate to 29.4% and increase the per student cost to $10,594. Finally, I find that a\nconditional cash transfer of $5,320 and $3,640 for two-year and four-year college enroll-\nment, respectively, attains the targeted postsecondary graduation rate of 60% by age\n29 and costs $16,569 per student with an estimated total cost of $2.3 billion. This is far\nin excess of the THECB's FY2018 operating budget of $807 million.\nStudent Debt and Housing Market Outcomes - Working Paper with Naila Rahman,\nRice Institute\nThe rapidly increasing levels of educational debt incurred by American students raises\nconcerns over the effects that the debt could have on their ability to achieve their long-\nterm goals, including homeownership. While amount of student loan debt held by house-\nholds has increased over the last few decades, homeownership rates among young\nadults have declined. Additionally, if borrowers are taking out more loans to attain higher\neducation, then higher future earnings could potentially mitigate the effect of student\nloan payments on home purchase. It is important to determine if the negative effect\nof student debt on housing demand is exceeded by the positive effect that comes from\nhigher future earnings. The purpose of this paper is to examine the relationship between\nstudent loan debt and housing demand.\nAnalyzing Unemployment Dynamics in the US Using a Structural VAR Approach with Sign Constraints - Lyon School of Economics, Spring 2013\nThis paper establishes dynamic features of the US labor market (1989-2012) within a\ntwo sector construction, non-construction framework using structural VAR with sign re-\nstrictions. I show that a heightened unemployment rate in the construction sector was\nan important driver of the unemployment dynamics during the Great Recession. This\ncan be explained by the cyclical sensitivity of this sector, with little reliance on invent-\nory inputs, and low separation rates of workers in the non-construction sector, so that\ndisplaced construction workers could not be easily reallocated.\nA Cross-National Flow Analysis of Unemployment in the UK, US, France and Spain - Madrid School of Eco-\nnomics, Spring 2012\nThis paper examines the unemployment dynamics in the UK, US, France and Spain in\norder to gauge the relative significance of unemployment inflows and outflows in pro-\nducing aggregate unemployment movements. I find that while unemployment inflows\nplay a dominant role in driving unemployment dynamics in the UK, France and the US,\nboth inflows and outflows are equally important for unemployment movements in Spain.\nThese results can be explained by institutional heterogeneities and interaction between\nnegative shocks and labor market rigidities across these countries.\nAwards\n2019\nJohn Kelly Award, Graduate Student Instructor\n2018, 2019\nSSRI Travel Grants\n2016 - 2017\nFellowship, Houston Education Research Consortium\n2016\nRobin Sickles Award for Best Performance in Econometrics Qualifying Exam\n2012 - 2013\nJean Jacques Laffont scholarship (full ride), Lyon School of Economics\n2011 - 2012\nLa Caixa scholarship (full ride), Madrid School of Economics\nOther Activities\nResearch Report for Houston Endowment and the Houston Independent\nSchool District\n* Houston Longitudinal Study on the Transition to College and Work (HLS). Part III: Labor\nMarket Analyses in Texas and Houston\n* Becker, L., Farooq, A., Serrano, M.J., Kim, J.Y. 2019.\n* This labor market report was produced in collaboration with the Houston Education\nResearch Consortium as part of a larger project for the Houston Endowment and the\nHouston ISD.\n* The analysis included producing forecasts of the labor market composition and es-\ntimation of demand and supply of workers with postsecondary credentials and STEM\nskills for Texas and Houston.\nConference Presentations\n* Southern Economics Association, October 2018.\n* Society of Labor Economists (SOLE), May 2019.\n* Western Economics Association, June 2019.\nResearch Assistant to Prof. Pradeep R. Menon, August 2017- April 2018.\nADDITIONAL INFORMATION\nImmigration Status\nU.S. Permanent Resident"}
{"resume_id":8,"text":"Data Scientist\nName : Ravi Sai Ajay Kumar Mandapati\nCurrent locations: Dallas,Texas, Zip Code : 75252\nEmail: ravi.ajaymandapati@gmail.com | Phone: +19452334678 | URL : https://www.linkedin.com/in/ajay-mandapati/\n\nProfessional Summary :\nExperienced Data Scientist with overall 6 years of extensive expertise in delivering end-to-end solutions in data science,\nmachine learning, and generative AI across multiple industries. Proficient in developing and deploying complex\npredictive models, implementing advanced natural language processing techniques, and leveraging generative AI for\ninnovative solutions. Adept at handling the full data science lifecycle, from data acquisition and preprocessing to\ndeploying scalable, cloud-based AI models using cutting-edge tools like Python, TensorFlow, PyTorch, AWS, and Azure.\nPossesses strong analytical, technical, and problem-solving skills, ensuring impactful results and alignment with\nbusiness goals.\n\nTechnical Skills :\nMachine Learning: Regression (Linear, Logistic, Ridge, Lasso), Classification (SVM, Naive Bayes, Decision Trees),\nClustering (K-Means, DBSCAN), Dimensionality Reduction (PCA, SVD), Ensemble Learning, Recommender Systems,\nPredictive Modeling\nDeep Learning: TensorFlow, PyTorch, Keras, CNNs, RNNs (LSTM), GANs, Autoencoders, Sequence-to-Sequence\nModels\nGenerative AI: GPT, BERT, Claude, Azure OpenAI, Amazon Bedrock, Mistral AI, LLaMA\nNLP: Sentiment Analysis, Text Classification, Chatbots, Transformers, Topic Modeling, Word2Vec, PhraseBERT\nStatistical Analysis: Hypothesis Testing, Sampling, ANOVA, EDA, Feature Engineering, A/B Testing\nProgramming: Python (Numpy, Pandas, Matplotlib, Seaborn, Scikit-learn), PySpark\nCloud Technologies: AWS (SageMaker, Lambda, Redshift, Athena), Azure OpenAI, Docker\nVisualization: Tableau, Matplotlib, Dash\n\nProfessional Experience\nData Scientist\nAT&T, Dallas, TX | Jun 2024 - Present\n*\nLed the architecture, design, and implementation of a highly advanced chatbot leveraging Large Language Models\n(LLMs), enabling users to interact dynamically and intuitively while enhancing overall user engagement.\n*\nDeployed and managed AI pipelines using Amazon Bedrock, ensuring high scalability, seamless model updates, and\nintegration with existing business processes.\n*\nUtilized LLaMA for advanced model adaptation and continuous learning from user interactions, significantly\nimproving chatbot response relevance and accuracy over time.\n*\nIntegrated Mistral AI for image recognition, allowing the chatbot to process visual inputs and provide contextually\naware responses and intelligent recommendations.\n*\nImplemented Anthropic's Claude to enhance the governance and ethical management of AI systems, mitigating\nrisks related to bias and ensuring fair interactions.\n*\nExplored and applied Azure OpenAI services to expand chatbot functionalities, enabling high-quality language\nunderstanding and generation capabilities.\n*\nDesigned and automated pipelines for data preprocessing, model training, validation, and deployment using AWS\nSageMaker, ensuring efficient resource utilization and quick turnaround times.\n*\nCollaborated with cross-functional teams to establish AI-driven solutions, enhancing business operations and\ncustomer satisfaction.\n\nEnvironment: Python (Numpy, Pandas, Matplotlib), Azure OpenAI, AWS SageMaker, Amazon Bedrock, LLaMA, Mistral\nAI, Anthropic's Claude, Docker, Git\n\nData Scientist\nNorthwestern Mutual, Dallas, TX  | Jul 2022 - May 2024\n*\nEngineered predictive models for credit risk assessment and loan default prediction, leveraging algorithms such\nas XGBoost, Random Forests, and logistic regression, enabling improved risk management and optimized credit\napproval processes.\n*\nDeveloped state-of-the-art NLP solutions using advanced models like BERT, DistilBERT, and PhraseBERT to analyze\ncustomer feedback and resolve misclassification in automated responses for financial inquiries, enhancing\ncustomer experience.\n*\nDeployed SARIMA and LSTM models for forecasting financial trends such as cash flow, revenue growth, and\nexpense projections, improving strategic planning and operational efficiency.\n*\nExtracted, cleaned, and analyzed large-scale financial datasets, including real-time market data from Twitter APIs,\nto derive actionable insights into market sentiment and public opinion on financial products and trends.\n*\nDesigned and managed AI training datasets optimized for BERT and XLNet models to classify and prioritize high-\nvalue client inquiries, achieving increased model precision and recall.\n*\nCollaborated with cross-functional teams to align data science initiatives with financial goals, such as improving\nportfolio management and identifying high-value opportunities.\n*\nCreated comprehensive dashboards and visualization tools using Tableau to track key financial performance\nindicators, risk metrics, and client engagement metrics, empowering stakeholders with actionable insights.\n*\nIntegrated AWS tools like Lambda and SageMaker into the model development pipeline for secure deployment\nand real-time monitoring of ML solutions in financial applications.\n\nEnvironment: Python (Numpy, Pandas, Seaborn), AWS (Lambda, Athena, Redshift, SageMaker), SQL, Tableau, NLP\n(Transformers, SpaCy, BERT, XLNet, DistilBERT)\n\nData Scientist | Value Labs, India| Nov 2020 - Jun 2022\n*\nDeveloped and implemented a hybrid BiLSTM-CNN model to map job descriptions to skill sets, improving\nrecommendation accuracy and simplifying workforce planning for clients.\n*\nApplied Latent Dirichlet Allocation (LDA) for document classification, leveraging topic modeling to enhance\ncategorization accuracy across large datasets.\n*\nEngineered advanced feature extraction techniques using Word Embeddings and Gaussian Mixture Models,\nenabling personalized skill recommendations.\n*\nBuilt Seq2Seq architectures to streamline skill suggestion systems, improving platform usability for end-users.\n*\nDeployed CNN-based autoencoders for text data representations, enhancing semantic relevance in text-matching\napplications.\n*\nApplied time series forecasting models to predict demand surges, reducing customer wait times and optimizing\noperational efficiency.\n*\nAutomated the ingestion and normalization of datasets from diverse sources using Python and PySpark, ensuring\ndata consistency and accuracy for downstream analysis.\n*\nDesigned scalable solutions for data manipulation and visualization, providing actionable insights for internal and\nclient stakeholders.\n\nEnvironment: Python (Scikit-learn, Numpy, Pandas, TensorFlow), PySpark, Hadoop, PostgreSQL, AWS\n\nData Scientist | HSBC, India | Sep 2018 - Oct 2020\n*\nTackled highly imbalanced datasets for fraud detection by implementing SMOTE, cost-sensitive algorithms, and\nadvanced preprocessing techniques.\n*\nImproved loan default prediction accuracy through ensemble modeling with Ridge, Lasso, and XGBoost, achieving\nindustry-leading performance metrics.\n*\nPerformed extensive feature engineering, including feature interaction, normalization, and encoding, to enhance\npredictive model quality and interpretability.\n*\nAutomated data pipelines and workflows using Spark, optimizing computational efficiency and reducing manual\nworkload.\n*\nDesigned Tableau dashboards to communicate critical insights to business users, supporting strategic decision-\nmaking processes.\n*\nEnhanced model performance evaluation with comprehensive metrics such as RMSE, MAE, ROC, and AUC, driving\niterative improvements.\n*\nIntegrated AWS Redshift for scalable data storage and analysis, ensuring the availability of large datasets for\nprocessing.\n*\nWorked cross-functionally with analytics teams to define KPIs and align project outcomes with business objectives.\n\nEnvironment: Python (Scikit-learn, Pandas), AWS Redshift, Spark, Tableau, XGBoost\n\nEducation :\nMasters: University: The University of Texas at Arlington, Data Science Machine Learning NLP, Dallas, TX - 2024\nBachelors:  University: University of Hyderabad, India - 2019"}
{"resume_id":9,"text":"Arvind Kumar Reddy\nData Scientist\n(272) 235-1189             arvind.k@jobtechsmail.com             LinkedIn\n\nSummary\n* Data Scientist with around 9 years of experience transforming complex datasets into actionable insights through advanced\nmodeling techniques and statistical analysis, driving strategic decision-making across multiple business domains.\n* Skilled in developing end-to-end ML solutions utilizing Python, Pandas, and SQL, with expertise in implementing ETL pipelines,\npredictive algorithms, and time series forecasting to solve challenging business problems.\n* Proficient in translating technical findings into compelling visualizations through Power BI, Tableau and Excel, enabling stakeholders\nto understand complex data patterns and make informed business decisions.\n* Experienced in mentoring technical teams within Agile environments, standardizing analytical methodologies, and establishing data\nquality protocols that enhance operational efficiency and foster data-driven cultures.\n\nSkills\n\nMethodologies: SDLC, Agile, Waterfall\nProgramming Languages: Python, R, SQL\nLibraries: Pandas, NumPy, Scikit-learn, TensorFlow, PyTorch, Keras, NLTK, Hugging Face Transformers, XGBoost, LightGBM\nDatabases & Big Data Technologies: Oracle, MySQL, MongoDB, SQL Server, Apache Spark, Hadoop, Databricks, Snowflake\nData Engineering & Processing: Apache Airflow, Talend, SSIS, Alteryx, ETL Pipelines, Data Wrangling\nData Modeling & schemas Design: Star Schema, Denormalized Schema\nData Visualization Tools: Tableau, Power BI, Excel (Advanced), Matplotlib, Seaborn\nCloud Platforms: Azure (Data Lake, Data Factory, Azure ML), AWS (EC2, S3, RedShift, SageMaker), GCP (BigQuery, Vertex AI, AutoML)\nMLOps & Deployment: MLflow, Kubeflow, Docker, Kubernetes, CI/CD (Jenkins, GitHub Actions), Terraform\nStatistical & Machine Learning Techniques: A/B Testing, Hypothesis Testing, Exploratory Data Analysis (EDA), Time Series Analysis,\nFeature Engineering, Model Optimization, Hyperparameter Tuning, Supervised & Unsupervised Learning\nMachine Learning Algorithms: Random Forest, Decision Trees, Linear & Logistic Regression, SVM, XGBoost, Large Language Models\n(LLM), Deep Learning (CNN, RNN, Transformers)\nVersion Control & DevOps: Git, GitHub, Jenkins\nSoft Skills: Stakeholder Engagement, Team Collaboration, Strong Communication, Problem-Solving, Attention to Detail\n\nEducation\nMaster of Science in Business Analytics | King's College, Pennsylvania\nMay 2024\nMaster of Business Administration (MBA) in Finance | Amity University, India\nSeptember 2017\nBachelor of Technology in Engineering | Shoolini University, India\nApril 2015\n\nWork Experience\nData Scientist | PNC, Pennsylvania\nJanuary 2024 - Current\n* Leveraged Python, Pandas, and SQL to collect and clean financial data from multiple sources, building ETL pipelines with Apache\nAirflow that automated data preparation workflows and reduced processing time by 30%.\n* Engineered novel behavioral and contextual features for credit card transaction data, and optimized machine learning models\n(XGBoost, Random Forest) with resampling techniques (SMOTE, ADASYN) and cost-sensitive learning, reducing false positive fraud\nalerts while maintaining fraud detection rates.\n* Created interactive financial dashboards with Power BI that visualized model outputs and risk metrics in near real-time, enabling\nstakeholders to make data-driven decisions that reduced operational risks by 20%.\n* Participated in Agile development methodology, working in two-week sprints with cross-functional teams to translate technical\nfindings into actionable insights and successfully integrate models into existing Oracle database systems.\n* Established monitoring procedures using Azure ML for deployed models ensuring regulatory compliance, while balancing model\nperformance with interpretability requirements through A/B testing and version control with Git and GitHub.\n* Utilized Explainable AI (SHAP, LIME) to analyze model decisions and refine the fraud detection system, and strategically optimized\nclassification thresholds, leading to a significant decrease in unnecessary customer alerts and improved customer satisfaction.\nData Scientist | Sage Softtech, India\nSeptember 2021 - December 2022\n* Analyzed sequential EHR data via Exploratory Data Analysis (EDA) to validate suitability for time-series forecasting, performing\nnecessary preprocessing (e.g., scaling, imputation) to enable accurate patient deterioration prediction.\n* Designed, built, and tuned an LSTM network (RNN, Python/TensorFlow/Keras) predicting patient deterioration, minimizing\nprediction error (loss function like MSE/Cross-Entropy) and achieving a 25% reduction in intervention time\nvia RMSE/MAE validation.\n* Developed a multi-class classification model (Python, Scikit-learn/XGBoost) using engineered EHR features to stratify patients\ninto distinct readmission risk categories (High, Medium, Low), facilitating proactive post-discharge care.\n* Implemented MLOps automation using MLflow within Databricks on AWS for predictive model (e.g., patient readmission)\nretraining/validation pipelines, ensuring model reliability and contributing to reduced readmission rates.\n* Leveraged Databricks on AWS (PySpark) to integrate and analyze large-scale, mixed healthcare datasets, developing an anomaly\ndetection system that flagged significant deviations in patient data (vitals, labs) for early clinical risk warning.\n* Applied Natural Language Processing (NLP) techniques (NLTK, Hugging Face Transformers) to analyze unstructured medical\nnotes, extracting valuable clinical insights that improved diagnostic accuracy.\n* Utilized R and Python (Scikit-learn, Statsmodels) for complex statistical analyses identifying key factors influencing patient\noutcomes, thereby refining clinical decision-making and personalizing treatment plans.\n* Spearheaded predictive modeling projects within an Agile framework, driving data-driven insights that enhanced patient care\nthrough timely interventions.\nLead Data Analyst | XL Dynamics Pvt Ltd, India\nJanuary 2019 - August 2021\n* Led analysis portfolio optimization, evaluating 200+ reports against business objectives, eliminating redundancies and refocusing\nefforts on actionable insights, resulting in 55% cost reduction and 60% efficiency improvement.\n* Applied comprehensive data quality protocols, standardizing procedures and conducting root cause analysis of anomalies,\nincreasing data trust and enabling analysts to focus on high-value analytical tasks.\n* Designed SQL-based data extraction frameworks and standardized analysis methodologies, creating reusable templates that\nreduced report creation time while maintaining consistency across analytical outputs.\n* Built predictive loan segmentation model using statistical clustering techniques to categorize financial products by risk factors,\nreducing high-risk loan misclassification.\n* Created automated dashboard solutions integrating multiple data sources, providing real-time business performance metrics that\nsupported executive decision-making and identified cost-saving opportunities.\n* Established structured training curriculum for data analysts covering SQL, analytical techniques, and visualization best practices,\nmentoring 35+ team members and reducing onboarding time.\nData Analyst | XL Dynamics Pvt Ltd, India\nMay 2015 - December 2018\n* Developed 25-30 critical business reports using Python, data modeling, and BI tools to provide insights on revenue trends and\ncustomer behavior, driving data-informed decisions.\n* Analyzed operational processes to identify bottlenecks, improving loan closing efficiency by 15% and determining profitable regions\nfor optimized resource allocation.\n* Extracted and analyzed loan data from SunSoft LOS and operational databases using advanced SQL techniques to uncover key\nmarket trends.\n* Designed impactful visualizations in Excel and Tableau to effectively communicate findings to stakeholders, enhancing\nunderstanding of loan performance metrics.\n* Executed comprehensive data quality checks and validation processes to ensure accuracy, consistency, and reliability of datasets,\nestablishing a strong foundation for trusted analytics.\n* Collaborated with senior analysts to enhance skills in advanced data analysis techniques, visualization strategies, and mortgage\nindustry knowledge.\n\nCertifications\n\n* Databricks Certified Machine Learning Professional\n* Machine Learning Fundamentals Micro-Credential by Alteryx\n* Alteryx Designer Core Micro-Credential: General Knowledge by Alteryx\n* Career Essentials in Data Analysis by Microsoft and LinkedIn\n* AWS Certified Planning a Machine Learning Project\n* Lean Six Sigma Foundations by LinkedIn"}
{"resume_id":10,"text":"Carlos Mendoza\nSenior AI/ML Engineer\nEmail: carlos.mendoza8899@gmail.com\nLocation: Jacksonville, FL 32218\nLinkedIn: www.linkedin.com/in/carlos-mendoza-zavala-90101322a\n1050\n-\n(385) 355\nContact:\n\nSummary\nInnovative AI/ML Engineer with 8+ years of hands-on experience driving impactful machine learning\nand artificial intelligence solutions across healthcare, biotechnology, and cloud services industries.\nExpertise in developing scalable AI models for predictive analytics, computer vision, and natural\nlanguage processing (NLP), with a proven track record of optimizing critical systems and improving\nbusiness outcomes. Proficient in leveraging cloud platforms (AWS) to build and deploy production-\nready ML pipelines, transforming complex challenges into high-performing, data-driven solutions.\nAdept at working with cross-functional teams to deliver cutting-edge AI strategies that align with\norganizational goals and fuel innovation.\nKey Skills\nMachine Learning & AI: Supervised/Unsupervised Learning, Deep Learning, NLP, Computer Vision,\nReinforcement Learning\nProgramming Languages: Python, R, Java, C++\nML Frameworks: TensorFlow, PyTorch, Keras, Scikit-learn, Hugging Face, OpenCV\nData Processing: Pandas, NumPy, Dask, Spark\nCloud & Big Data: AWS (SageMaker, EC2, S3), Google Cloud, Azure, Hadoop\nModel Deployment: Docker, Kubernetes, Flask, FastAPI, TensorFlow Serving\nDevOps/CI-CD Tools: Jenkins, Git, MLflow, Airflow, Kubernetes\nData Engineering: SQL, MongoDB, Apache Spark, Kafka\nAgile/DevOps Practices: Scrum, Kanban, CI/CD Pipelines\nProfessional Experience\nAI/ML Engineer\nSnapCare - [Remote | Atlanta, GA]\n04/2021 - Present\nLed the development and deployment of AI-driven predictive models for a healthcare platform,\nincreasing the accuracy of patient outcome forecasting by 25%, leveraging XGBoost, Neural Networks,\nand time-series data analysis.\nDeveloped NLP pipelines for extracting and analyzing critical data from unstructured medical records,\nimproving data analysis efficiency by 30% and enabling faster decision-making for healthcare\nprofessionals.\nDesigned and implemented an AI-driven chatbot using Python, TensorFlow, and Dialogflow, improving\ncustomer engagement and support efficiency.\nSpearheaded the implementation of an anomaly detection system to identify and flag irregular patient\ndata and treatment outcomes, significantly reducing errors in diagnosis and treatment planning.\nCollaborated closely with healthcare providers and data engineers to ensure the AI models aligned\nwith medical compliance standards, improving HIPAA compliance and data security.\nDesigned end-to-end AI/ML pipelines for real-time data processing and model deployment in AWS\nSageMaker and Lambda, reducing operational costs by 20% while maintaining 99.9% system uptime.\nLed the integration of a computer vision system to analyze and classify medical images, increasing\ndiagnostic accuracy by 18% and reducing manual review times.\nSenior Machine Learning Engineer\nGenmab - [Remote | Princeton, NJ]\n03/2019 - 03/2021\nSpearheaded the development of AI/ML models to support biotech research and clinical trials at\nGenmab, a leading biotechnology and healthcare company.\nApplied deep learning techniques for genomic data analysis, increasing drug discovery success rates by\n35% through optimized candidate selection processes.\nDesigned and deployed a computer vision system using CNNs for cancer detection from medical\nimages, improving diagnostic accuracy by 20%.\nCollaborated with bioinformatics teams to ensure machine learning models aligned with FDA\nregulations and internal compliance standards, accelerating clinical trial insights.\nMachine Learning Engineer\nAmazon Web Services (AWS) - [Remote | Austin, TX]\n11/2017 - 02/2019\nWorked on the development of high-performance AI/ML models for AWS, contributing to core\nproducts like AWS Lex, AWS Rekognition, and AWS Transcribe.\nDeveloped and optimized AI pipelines in AWS SageMaker, improving model training efficiency for AWS\ncustomers by 30% and leading to increased adoption of AWS AI-powered services.\nDeployed scalable machine learning solutions using AWS Lambda, Docker, and Kubernetes, ensuring\nseamless scaling and efficient resource utilization.\nImproved natural language processing (NLP) models for AWS Comprehend, resulting in more accurate\nvoice-to-text conversions and better text analysis for enterprise customers.\nData Scientist\nOpynHealth - [San Francisco, CA]\n01/2015 - 10/2017\nDeveloped AI-powered predictive models for OpynHealth, a healthcare technology company, to\nforecast patient readmission risks, improving prediction accuracy by 20%.\nBuilt an automated machine learning pipeline for analyzing electronic health records (EHRs), increasing\ndata processing efficiency by 25%, which streamlined healthcare workflows.\nCreated a personalized healthcare recommendation engine using machine learning algorithms,\noptimizing patient treatment plans and reducing the cost of care by 15%.\nEnsured strict HIPAA compliance while working with sensitive healthcare data and deploying models in\nproduction environments.\nEducation\nJacksonville University\nB.S. Computer Science, 2014"}
